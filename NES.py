import numpy as np
import matplotlib.animation as animation
import matplotlib.pyplot as plt
import tqdm

np.random.seed(42)

def natural_evolution_strategy(optimization_problem, population_size=100, num_iterations=100, lr_mu=0.1, lr_sigma=0.1, initial_mu_sigma=None, do_NES=True):
    "returns mus, scores as lists. The initial positions should be arrays"

    ### break optimization problem into it's parts
    negative_fitness_function=optimization_problem.neg_fitness_function
    arguments=optimization_problem.arguments

    if initial_mu_sigma:
        mu,sigma=initial_mu_sigma
    else:
        # Initialize a random solution
        mu = np.random.randn(arguments) #random positions in N(0,1)
        #starting sigma vector (arguments,)
        sigma=np.random.uniform(0.1, 1, arguments) 

    mus=[mu] ### the mu over time
    scores=[negative_fitness_function(mu)]

    np.random.seed(42)

    for _ in tqdm.tqdm(range(num_iterations), desc=f"running NES for {optimization_problem.name}"):

        #Generate a population of candidate solutions
        z = np.random.randn(population_size, len(mu))  #(population_size x length of mu (=arguments in optimization problem))
        individuals = mu + sigma * z #translate the sampled distributions to be N(\mu,\sigma)

        neg_fitness = np.array([negative_fitness_function(ind) for ind in individuals])
        
        neg_fitness = np.squeeze(neg_fitness) ### due to the shape the functions return
        
        if optimization_problem.name=="schaffer" or optimization_problem.name=="sphere":
            ##### take only the best fitness cases (generated by Copilot) #########
            # Calculate the number of top elements to select
            num_top = int(0.25 * len(neg_fitness))

            # Find the indices that would sort the fitness array in descending order
            indices = np.argsort(neg_fitness)[::-1] ##reverses the list so that descending order

            # Select the top of indices
            top_indices = indices[:num_top]

            # Select the corresponding pairs from fitness and z
            neg_fitness = neg_fitness[top_indices]
            z = z[top_indices]

        ### the formulas in the report -> sampled gradients: ###
        grd_mu_log_ps=z/sigma
        grd_sig_log_ps=z**2/sigma-np.reciprocal(sigma)

        ### calculating the SGD gradient approx ###
        hat_grd_mu_F=np.mean(grd_mu_log_ps*neg_fitness[:,None], axis=0) #sum away the rows
        hat_grd_sig_F=np.mean(grd_sig_log_ps*neg_fitness[:,None], axis=0)

        if do_NES: 
            step_mu=sigma**2 * hat_grd_mu_F
            step_sig=1/2 * sigma**2 * hat_grd_sig_F
        else:
            step_mu=hat_grd_mu_F 
            step_sig=hat_grd_sig_F

        # Update the solution
        mu = mu - lr_mu * step_mu
        sigma = sigma - lr_sigma * step_sig

        if sigma.any() < 0: ## never used ##
            print("sigma < 0!!!")
            print(sigma)
            raise ValueError

        mus.append(mu)
        scores.append(negative_fitness_function(mu))

    return mus, scores


class Optimization_problem:
    def __init__(self, neg_fitness_function, arguments, name=None):
        self.neg_fitness_function = neg_fitness_function
        self.arguments=arguments
        self.name=name
    
### test case
def negative_fitness_function(position):
    "this is something which should be minimized! return a list to be consistent with the other benchmarking functions"
    return [(10-position[0])**2 + (2-position[1])**2]

def visualize_optimization_process(solutions:list, fitness_function, make_contour=True, global_opt:list=None, label=None):
    "Generated with Copilot partly"

    # Convert solutions to numpy array for easier indexing
    solutions = np.array(solutions)

    if make_contour:
        # Increase the default font size
        plt.rc('font', size=20) 
        # Create a grid of points
        if global_opt:
            x = np.linspace(min(global_opt[0], min(solutions[:, 0]))-2, max(global_opt[0],max(solutions[:, 0]))+2, 100)
            y = np.linspace(min(global_opt[1], min(solutions[:, 1]))-2, max(global_opt[1],max(solutions[:, 1]))+2, 100)
        X, Y = np.meshgrid(x, y)

        # Calculate fitness for each point on the grid
        Z = np.array([fitness_function([x, y]) for x, y in zip(np.ravel(X), np.ravel(Y))])
        Z = Z.reshape(X.shape)

        # Plot the contour map
        plt.contourf(X, Y, Z, levels=20, cmap='viridis')
        plt.colorbar()

    # Plot the solutions
    plt.plot(solutions[:, 0], solutions[:, 1], '.-', label=f"{label}")
    plt.text(solutions[-1, 0], solutions[-1, 1], f'End {label}')
    if global_opt:
        plt.text(solutions[0, 0], solutions[0, 1], f'Start', color='blue') 
        plt.plot(global_opt[0], global_opt[1], 'y*', markersize=5, label='optimum')

if __name__ == "__main__": 
    ### a test case ##

    lr_mu=0.1
    lr_sigma=0.1 
    mus, scores=natural_evolution_strategy(Optimization_problem(negative_fitness_function,2), 100, 100, lr_mu, lr_sigma)

    print(mus[-1])
